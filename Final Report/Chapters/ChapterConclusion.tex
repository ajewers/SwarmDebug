% Chapter Conclusion

\chapter[Conclusions and Future Work]{Conclusions and Future Work} % Main chapter title

\label{ChapterConclusion} % For referencing the chapter elsewhere, use \ref{ChapterConclusion} 

%----------------------------------------------------------------------------------------

\section{Conclusion}

The stated aim of this project was to create a tool capable of aiding a developer in the task of debugging a robot swarm, by improving their access to, and interpretation of, previously unavailable internal robot state and sensor data. This has been achieved through the implementation of a software application and associated infrastructure system which retrieves this data wirelessly, fuses it with tracking data and a live video feed, and presents it to the user in a number of formats, including graphical formats rendered using augmented reality techniques, all in real time. The system was designed and implemented with modularity and portability in mind, using a standardised packet format and requiring no specific hardware on the robot side, to ensure it can be easily integrated with many robot platforms.

Following its implementation the software underwent testing to ensure its correct and robust operation, and was then evaluated by potential users, including a group with specific experience of swarm robotics research and development. This evaluation included a simulated debugging task in an attempt to assess the value of the system in its intended context. The results of this evaluation indicate that the system is both intuitive to use, and useful in a debugging scenario, with the majority of the participants indicating that access to, and visualisation of, internal robot data was helpful in solving the problem. One participant from the non-expert group went as far as to question how debugging robot swarms was possible without a system of this nature. The evaluation was however limited by its small sample size and simulated conditions, and in future the system could benefit from use in a range of genuine debugging situations.

One of the key motivations for the project was the desire to create a system which is practical, usable, and offers real benefit to swarm robotics developers and researchers. A number of members of the York Robotics Laboratory have commented on the potential usefulness of the system, and expressed a desire to use it in practice. It is therefore hoped that the application will continue to be used and developed within the YRL to aid in swarm robotics research and development efforts. Taking a wider view, it is also hoped that the system, or some possible future iteration, might be useful to other swarm robotics researchers outside of the YRL. For this reason the source code is made freely available, and can be found at \url{https://github.com/ajewers/SwarmDebug}.

A wide range of areas for possible future expansion of the system have been identified, and are discussed in section \ref{FutureWork}. This first version lays the groundwork for much of that potential development, whilst also achieving all the objectives identified at the start of the project. The application software has been shown to satisfy all of the core requirements of the functional specification fully, as well as satisfying a number of the secondary requirements at least in part. Considering this, and the largely positive nature of the feedback received during evaluation, the project can be considered successful.

%----------------------------------------------------------------------------------------

\section{System Limitations}

In spite of the projects success in achieving the aims and producing a working system, a number of limitations inherent to the systems nature have been identified and are summarised here.

\begin{description}
 \item [Effect on robot behaviour.] In order to receive data from the robots, the system requires that   additions are made to the robot's controller code to report this data. Any time code is added to the robot's controller it has the potential to effect the behaviour in unforeseen ways, even if the added code has no direct effect on the code controlling decision making. This is because time is required to execute the data reporting code, and this can have a knock on effect on the execution of the rest of the code. This is most dangerous when deploying a swarm system, as the developer may choose to remove all of the data reporting code prior to producing the `release' version of the system, as this code is debugging focused, and not needed in a final version once correct operation has been established. If there is a significant amount of data reporting code, removing it all could have a non-negligible effect on the robots behaviour, leading to potential unpredictability. In order to mitigate this problem as much as possible the robot side code has been implemented to require as little time as possible to transmit data packets.
 
\item [WiFi network requirement.] The system requires that all robots be connected to one WiFi network in order to operate correctly. This means the current system is limited to robots which can support WiFi, and requires that the network infrastructure be in place. Considering the system's intention as a debugging tool, and therefore its likely use within laboratories, WiFi network infrastructure is not an unreasonable requirement. However not all robot platforms support WiFi, hence this system in its current state is limited to robot platforms that do. Section \ref{HardwareExpansion} discusses extending the system in future to support a further wireless protocol such as Bluetooth.

\item [Fixed Camera Viewpoint.] The video based portion of the system has been implemented to function with the video captured from a fixed viewpoint, with an overhead, birds-eye view of the robots. The system cannot correctly augment the video if it is captured from any other angle, even though the tag detection algorithm can cope with a range of angles. The system is therefore limited to consider and augment the space in only be two dimensions, and cannot display height-related data in any way. Many consider a three-dimensional reference frame to be a requirement of a true augmented-reality system \cite{Azuma:1997}, cite{Billinghurst:2014}, and this system would have to be modified to support a non-specific viewpoint in order to work with modern augmented reality hardware.
\end{description}

%----------------------------------------------------------------------------------------

\section{Future Work} \label{FutureWork}

This project has focused on developing the first version of this swarm robotics debugging system. Responses to the initial user survey, and anecdotal comments from members of the York Robotics Laboratory, have indicated that there is an interest in seeing the development of this system continue and its capabilities expanded to a number of new areas. This section discusses potential directions that future development could take, and considers how these might benefit users of the system. Some key areas for potential development are as follows:

\begin{itemize}
 \item Implementation of additional debugging features.
 \item Implementation of analytical and experiment-enabling features, such as macro level analysis and data export, leading to the system becoming a full swarm robotics research and development platform.
 \item Expansion of the target robot platforms and supported hardware.
 \item Integration with native augmented reality hardware.
\end{itemize}

%----------------------------------------------------------------------------------------

\subsection{Additional Debugging Features}

The simplest way to extend the system further would be to implement support for a number of additional debugging features. This could include support for data from different types of sensor, as well as different methods for interpreting or displaying data. Support could be added for receiving and visualising higher level data types such as directional vectors, which could be useful in a number of tasks. An example use for this would be to display a robot's `\textit{diffusion vector}' whilst running a dispersion behaviour. This vector indicates the direction the robot believes it needs to move in order to increase its distance from nearby objects. Rendering direction vectors within the visualiser, positioned to match the relevant robot's location and orientation would be a good use of the visualiser, and would remove a level on indirection for the viewer when trying to relate this vector data back to the robots current orientation, which may be changing rapidly. Adding support for additional data types would however require new packet types to be defined, and new elements to be added to the data model, as well as new visualisation code. For a single addition this is not an overwhelming task, however as the system continues to grow this could become cumbersome.

It is almost impossible to predict all of the possible types of data, or possible ways of visualising this data, that might be desired by users of the system. Each swarm behaviour will have its own unique features and needs, and each different robot platform will have a different set of sensors and actuators. Therefore trying to create specific visualisations that cover all the possibilities is simply infeasible. Instead an approach that might yield much more flexible and useful results could be to incorporate a scripting language, and develop an API for the visualiser, allowing users to write simple scripts which define custom visualisations. The user could then save these scripts and import them into the application, telling the visualiser to run the scripts combined with data from the data model to generate the custom visualisations. The custom data portion of the data model could be extended to support marginally more complex data structures such as arrays, allowing these custom visualisations to be more flexible.

These scripts could be written in a language such as \textit{Python}, or \textit{LUA}, and called from within the application using a framework such as `\textit{Boost.Python}'. One downside to this approach would be the requirement for users to have some knowledge of programming, however considering the target users of the system are robotics developers and researchers, it seems unlikely that they would not be familiar with programming concepts. A well designed API could help to keep the requirements for these visualisation scripts simple, reducing the burden on the user. The incorporation of a scripting system of this nature would massively reduce the complexity of extending the application, and would have the added benefit of allowing scripts to be shared, potentially leading to collaborative efforts to extend the system.

%----------------------------------------------------------------------------------------

\subsection{Towards a Full Swarm Robotics Research and Development Platform}

The system could be further developed to become a full platform for swarm robotics research and development, which would be used not only for debugging swarm behaviours but also for running and monitoring swarm experiments and collecting experimental data. A number of additional features would be key to making this step forward. Firstly an organised method for managing experiment `runs', coupled with a more robust data logging system, could be implemented. This would allow the user to specify when an experiment run was taking place, and automatically export organised experimental data related to each experiment run. This data could then be used when analysing the results of the experiment. The user could also control which elements of the data should be recorded, to ensure relevance to their specific experiment.

The system could also be extended to support bi-directional communication, in order to send simple control commands to the robots. This could include commands to start, stop, pause and restart specific behaviours or experiments. When coupled with the improved data logging and experiment organisation this could allow a user to start and stop experiment runs without having to interact directly with each robot, allowing them all to be started simultaneously. This bi-directional communication could also send other types of commands to the robots, potentially experiment-specific, user-defined ones, that might give high level directives to control swarm behaviour.

The addition of higher level analytical features, focusing on the behaviour of the overall swarm rather than the individual robots, could also contribute to the system's use as a swarm robotics platform. These could analyse useful swarm-level parameters such as aggregate position over time, distributions such as spread and skew, and grouped state information (how many robots in each state, average time spent in each state, etc.). Robots exhibiting behaviour significantly different from the average could also be highlighted, as a way to detect problems. These higher level analytical features could help a user to test and verify novel swarm behaviours more easily. This extra analysis could also could be coupled with new visualisations, giving the user immediate visual indicators of high level parameters, and their change over time.

The augmented video feed is one of the key features of the application, and provides scope for many possible extensions. Code could be implemented to allow video to be recorded directly from this feed, allowing a user to watch back experiment runs and replay specific sections, perhaps gaining insight into specific interactions or trends within the swarms behaviour. Exported video could also be used for demonstration purposes, with the graphical augmentations adding context for a viewer who is less knowledgeable regarding the swarms inner workings.

%----------------------------------------------------------------------------------------

\subsection{Expansion of Supported Hardware} \label{HardwareExpansion}

Wherever possible this system has been implemented with portability in mind, with the aim of allowing different robots to be easily incorporated into the system. The use of ArUco tags to achieve robot tracking means this portion of the system is fully independent of the robots, requiring no specific hardware, only simple printed paper tags as a minimum. The robot side code has also been designed to be as portable as possible, however it still has two main requirements which are fairly specific; the robots must be running linux, and they must be able to connect to a WiFi network. WiFi connectivity is not always common amongst smaller robot platforms, with many instead utilising Bluetooth to achieve wireless communications. Therefore implementing Bluetooth communication support within the application, alongside WiFi, and adding Bluetooth support to the robot side API could significantly increase the number of robotic platforms on which the system could be used.

Currently the system supports only one specific machine vision camera. A major challenge in the future development of this system will be to find a way to support multiple different camera set ups, with potentially bespoke drivers and APIs, in a way which requires minimal or ideally no changes to the code. One way to achieve this might be to extend the application to be able to receive the video feed via a network, leaving it up to the user to implement a system which obtains the video from their specific camera set up and sends it to the application. This would also allow the application to be run on a machine without a physical connection to the camera hardware. One of the main barriers to this approach, and the reason it was not included in this implementation, is the high bandwidth required to transmit the video feed.

%----------------------------------------------------------------------------------------

\subsection{Integration with Dedicated Augmented Reality Hardware}

In recent years there has been rapid development in augmented reality technology, with the technology beginning to find its way into real world applications. Dedicated hardware platforms such as Microsoft HoloLens, Google Glass, and Google Cardboard have given consumers a first taste of real, perspective based, head mounted, augmented reality. By contrast, the version of augmented reality used in this project is simplistic, and fairly limited. In the future, a combination of a swarm robotics tool such as the system developed in this project, and dedicated AR hardware, could lead to much more immersive and effective human-robot interaction. The data visualisation could be developed to support full 3D, and utilise the motion tracking abilities of these hardware platforms,and the orientation calculation features built into the ArUco system, to render the augmentations from any perspective. This report firmly believes that, considering the immense potential of augmented reality systems when coupled with robotics, the use of AR-based tools when working with robots will become commonplace.

%----------------------------------------------------------------------------------------

\subsection{Integration with Tablet Application}

Another project being completed at the University of York, simultaneously with this project, has implemented a similar swarm debugging system as an Android app for a tablet computer. The main aim of this approach is to allow the user to be more mobile whilst using the system, so that they can interact with the swarm in a hands on fashion whilst also having immediate access to internal data from the robots. In the future the tablet application could be integrated with the application developed in this project, to form a single platform, with the tablet acting as a satellite terminal through which the user can access the system.

%----------------------------------------------------------------------------------------