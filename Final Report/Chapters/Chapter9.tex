% Chapter 9

\chapter[Implementation]{Implementation} % Main chapter title

\label{Chapter9} % For referencing the chapter elsewhere, use \ref{Chapter9} 

%----------------------------------------------------------------------------------------

This section gives details of the implementation phase of the project, including descriptions of how parts of the system function, information regarding the development process and explanations for some of the key decisions made regarding the implementation.

\section{Overview}
The system was implemented closely following the design laid out in chapter \ref{Chapter8}, and is comprised of two parts; the main computer software application (referred to henceforth as `the application') and a collection of software routines designed to be included within the code on the robots, which form an `application programming interface' (API) . This API (referred to as the 'robot side code' or 'robot side API') provides the developer with functions to send data from the robot back to the application, and contains routines for handling the networking requirements to achieve this, and for correctly formatting the data. Details of this robot side API are provided in section \ref{RobotSide}. Both parts of the system implementation are fully independent. The application can be run on its own and receive data from another source, provided that this data correctly follows the format outlined in section \ref{DataTransferFormat}. The robot side code could be used to send data to another host, provided that the robot used the correct target IP address and port number, and again provided that the host could correctly interpret the data format. The application is the much larger of the two system parts, and therefore will be the focus of the majority of this implementation chapter.

Figure \ref{fig:UI} shows the user interface for the application as it is seen at start-up. This can be compared to figures \ref{fig:UILayout} and \ref{fig:UIExample} to see the relationship to the user interface design. The key features of the application can be seen in this image, including the video feed augmented with information about the three visible robots, including position, direction, ID number, and for the selected robot name and current state. The robot list panel is also visible on the right hand side, showing the IDs and names of the known robots, and which is currently selected. Finally the data panel can be seen at the bottom of the application, currently set to the overview tab, providing more detailed information about the selected robot. More detailed information about the implementation of the UI itself can be found in section \ref{UserInterfaceImplementation}.

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{Figures/ApplicationScreenshotOverview.png}
	\decoRule
	\caption[Application User Interface]{The user interface for the application.}
	\label{fig:UI}
\end{figure}

\section{Application Framework Selection}
Developing computer software applications often involves implementing a large amount of the same low level functionality, regardless of the application. This includes low level back end functionality such as event management and dissemination, standard constructs such as timers and threads, as well as standard user interface elements such as windows, menus, panels, lists, tables, buttons and many more. It is clear that these functionalities are independent of the purpose of the application being developed, and implementing them from scratch for each new application would require a huge amount of time, and therefore be extraordinarily inefficient. For this reason the vast majority of modern software is created using some kind of application programming framework. The purpose of these frameworks is to provide the common, low level application functionality in the form of an API, which the developer can then leverage to develop their specific application. The API will usually include a number of classes to define the common UI elements, and a hierarchical tree- or node-based structure the developer can populate with these individual elements to create their desired UI. Most computer applications are event driven; when the user provides some kind of input such as a click or a key press an event is generated. The event generated will be different depending on which UI component the user was interacting with. This event then needs to be disseminated through the application in order to trigger the correct functionality. Most application programming interfaces will handle the generation and dissemination of these events, allowing the developer to `\textit{register}' code to be executed in response to specific events. The developer can therefore simply focus on implementing the functionality that is unique to their application.

Most modern application frameworks contain a wide variety of features in addition to those responsible for the UI and event management. These can include everything from low level components such as timers, input/output (I/O) interfaces, and networking components, to higher level multimedia handlers such as video and audio players, and rich HTML viewers. Frameworks also might include classes for creating data models which can be easily mapped to more complex user interface elements such as responsive tables.

Selecting an appropriate framework to use as the basis for this application was one of the first steps in the implementation process. All of the available frameworks have different benefits and limitations, and target a variety of different platforms, operating systems, and languages. During the design phase it was determined that the application should be implemented in C++, for reasons discussed in section \ref{SoftwareArchitectureDesign}. This therefore eliminated a number of frameworks, such as the Oracle Application Development Framework which is specific to the Java language, and Microsoft's .NET framework, which is C\# specific. The application also needed to run on the server connected to the tracking camera, which runs a Linux operating system. This therefore ruled out any windows specific framework, including one of the most widely used C++ frameworks, the Microsoft Foundation Class Library (MFC).

\subsection{The Qt Framework}
It was determined that the `\textit{Qt}' application framework was the most suitable for this application, as it provides support for cross-platform compilation, including Linux, and was implemented natively in C++. A number of factors, in addition to the target platform and language, influenced this decision. The framework is fairly widely used, and therefore has a well-tested, refined, and mature API, with a good body of documentation available. It provides a comprehensive library of classes for a range of common application functionalities, including GUI front-end and low level back-end components. The framework also includes built in support for multi-threading, and a structured `\textit{signals and slots}' system for sending event notification and moving data between components and across threads. For an application such as this, with a number of external data sources in addition to the user input and a multi-threaded design, this was determined to be a highly beneficial feature which could reduce development complexity significantly. Furthermore previous experience with Qt outside of this project had been positive, and meant a smaller learning curve would be necessary to get started developing the application. For non-commercial projects Qt is available free of charge, making it a good fit for an academic project.

The following primary features of the Qt application framework were used within this project:

\begin{itemize}
 \item Standard GUI components and layout management classes used to create the majority of the user interface.
 \item Event management system to handle user input events.
 \item `\textit{QTimer}' component to implement timers and recurring events, such as camera polling.
 \item `\textit{QThread}' API to partition the application components into threads.
 \item `\textit{Signals and Slots}' system for inter-component and inter-thread signalling and data transfer.
 \item `\textit{QPainter}' component for rendering custom user interface elements.
\end{itemize}

%----------------------------------------------------------------------------------------

\section{Source Code}
The system source code is implemented in two groups of files; a collection of C++ source and header files which make up the main application, and a smaller collection of C++ source and header files which handle the robot-side portion of the system. In addition to its source files the main application also relies on a number of other files which are used by the Qt system to define the user interface layout, and manage the build process. Table \ref{tab:CodeFiles} details the names and purposes of all files within the main application. Table \ref{tab:RobotCodeFiles} details the files that make up the robot side API.

\begin{longtable}{ l p{10cm} }
\caption[Application Code Files]{Source code and tertiary files that make up the main application.}\\
 File & Purpose\\ 
 \hline
 main.cpp & The entry point for the application. Instantiates the MainWindow class.\\
 mainwindow.cpp, .h & The core class, contains the entry point for the application and controls the set up and tear down processes and handles UI events within the main window.\\
 mainwindow.ui & Describes the user interface layout in a XML-like format. Used by the Qt framework to construct the UI.\\
 datamodel.cpp, .h & The top level class encapsulating the full data model. Maintains a list of RobotData objects.\\
 robotdata.cpp, .h & A class encapsulating the data of a single robot, including ID, position, state, sensor data and user data.\\
 datathread.cpp, .h & This class contains all routines for receiving data from the robots via wifi, and is designed to be run on a thread of its own.\\
 cameracontroller.cpp, .h & The high level class encapsulating the routines and data related to the tracking camera. This class is designed to be independent of the camera hardware being used.\\
 machinevision.cpp, .h & A lower level class encapsulating routines for interfacing with the camera hardware.\\
 visualiser.cpp, .h & This class encapsulates the visualiser GUI object, and is implemented to conform the Qt framework as a custom extension to the QWidget class. Also contains routines for applying the video augmentations as per the current visualiser settings.\\
 viselement.h & Contains an abstract class definition for a single visualiser settings element. These elements are used to define how specific elements of the video augmentation are rendered, and also contain settings and variables relevant to this task. \\
 vis*.cpp, .h, .c & Classes beginning with the `\textit{vis}' prefix derive from the VisElement abstract class and contain routines for rendering the visualisation for one type of data. The latter part of the class name identifies which data type is targeted.\\
 irdataview.cpp, .h & A custom GUI object, derived from QWidget, which displays the raw IR sensor data as a bar graph in the data window.\\
 settings.cpp, .h & Encapsulates the general application settings and routines for changing their values according to user input.\\
 log.cpp, .h & Encapsulates the routines for logging events and data to text files.\\
 util.cpp, .h & Contains static utility functions used in various places throughout the application code.\\
 *settingsdialog.cpp, .h & Classes ending with the `\textit{settingsdialog}' suffix describe dialog windows for adjusting the settings related to the visualisation of specific data types, identified by the first part of the class name.\\
 robotinfodialog.cpp, .h & Defines a dialog window which displays meta information about a specific robot, and provides controls to delete that robots data from the data model.\\
 addidmappingdialog.cpp, .h & Defines a dialog window which can be used to add a non-standard ID mapping.\\
 testingwindow.cpp, .h & Defines a dialog window for running and displaying the results of the data model unit tests.\\
 appconfig.h & Contains pre-processor definitions for controlling inclusion/exclusion of code segments.\\
 SwarmDebug.pro & Used by the Qt framework to build the application. Directs to the necessary code files and libraries.\\
 \bottomrule\\
	
 \label{tab:CodeFiles}
\end{longtable}

\begin{longtable}{ l p{10cm} }
\caption[Robot-side Code Files]{Source code files that make up the robot side API.}\\
 File & Purpose\\
 \hline
 debug\_network.cpp & This file encapsulates networking functionality for communicating with the debugging system. Contains routines for sending data of specific types, as well as for sending raw packets.\\
 debug\_network.h & Header file for the debugging system network interface. Also contains definitions for data type identifiers.\\
 \bottomrule\\
	
 \label{tab:RobotCodeFiles}
\end{longtable}

%----------------------------------------------------------------------------------------

\section{Data Model} \label{DataModel}
The data model forms the core of the `\textit{unseen}' or back end code for the application - code that does not directly influence the interface seen by the user. For each robot being tracked by the system, the data model is required to store the information related to that robot. This is done in a structured manner, such that other parts of the application can query specific data within the model easily. The contents of the data model are updated whenever new data arrives, and are consulted when rendering the user interface. Section \ref{DataModelDesign} describes the design of the data model and it's hierarchical structure. The implementation follows this design closely, with the \textit{DataModel} class contained in \textit{datamodel.cpp / .h} encapsulating the top level data model container, and the individual robot data object encapsulated in the \textit{RobotData} class in \textit{robotdata.cpp / .h}. 

The \textit{DataModel} class uses a standard C++ vector to maintain a list of \textit{RobotData} objects. Each of these \textit{RobotData} objects describes one robot that is currently known to the system. The list is ordered based on the robot ID of each entry, and is sorted after each new insertion. The \textit{DataModel} class provides convenience functions for data retrieval functionality, such as retrieving a robot's data given its ID. It also provides a function for entering new data into the model, which inherits the properties of a `\textit{slot}' within the Qt framework, allowing it to be called from other threads. Each new data packet received from the network is routed to this slot, as well as position data obtained from the tracking system. All data is supplied in the form of a string in one of the packet formats described in section \ref{DataTransferFormat}. Code within the data model class then handles interpreting the string, determining its purpose and the source robot, separating out the data and updating the relevant robot within the model. Whenever data arrives from a previously unknown robot, this code handles creating a new \textit{RobotData} object, and adding it to the list.

The \textit{RobotData} class encapsulates the data for a single robot. This includes a large number of individual data points, in a number of different formats. Table \ref{tab:RobotDataContents} describes these data points.

\begin{longtable}{ l l p{8cm} }
\caption[Robot Data Contents]{The contents of the \textit{RobotData} class.}\\
 \hline
 Data Point & Type & Description\\
 \hline\\
 Robot ID & Integer & The numerical ID of the robot, used by the tracking system and when transmitting data.\\
 Robot Name & String & The name associated with this robot. Set by the user when programming the robot, and reported in watchdog packets. \\
 State & String & The current state of the robot. \\
 Known States & List of strings & A list of all states the robot has previously reported. \\
 Position & 2D Vector & The current position of the robot expressed as a proportional coordinate vector. \\
 Angle & Integer & The current angle of the robot in degrees. \\
 Colour & OpenCV Scalar & The colour used for this robot in the visualiser, if colours are enabled, expressed as an OpenCV Scalar struct in RGB format. \\
 IR Data & Array of Integer & The most recent IR sensor readings for this robot. One value per sensor. \\
 Background IR Data & Array of Integer & The most recent background IR sensor readings. One value per sensor. \\
 Custom Data & Key Value Map & All custom user data. Stored as a map of key value pairs. \\
 
 \label{tab:RobotDataContents}
\end{longtable}

In addition to this data, the class also maintains a list of recent state transitions, and a short term history of the robot's position. The state transition list uses a custom structure to store the state before the transition, the state after the transition, and the time the transition occurred. A fixed size array of these custom structures is maintained and updated each time a state transition occurs, acting as a first-in first-out (FIFO) queue. The position history is stored in a similar fashion, using a fixed size array of coordinate pairs, which is updated every Nth position update. This interval can be configured by the user, with a lower interval giving a higher resolution but a shorter history, and vice versa for a higher interval.

%----------------------------------------------------------------------------------------

\section{Video Feed and Tracking System} \label{VideoFeedAndTrackingSystem}
The code relating to retrieving images from the machine vision camera and running the ARuCo tag tracking algorithm can be found in the files \textit{cameracontroller.cpp / .h} and \textit{machinevision.cpp / .h}. This code is run on the separate camera thread, in order to maximise application performance and ensure responsiveness in the event that the camera driver blocks execution whilst retrieving the next frame. The \textit{CameraController} class handles the higher level operations such as running a timer to periodically poll the camera driver for the next image, supplying the driver with the correct dimensions for the image to be resized to in order to fit within the available space in the UI, and converting the image itself and the tracking data into formats which can be passed back to the main thread and used in the UI and data model respectively. The application threading is handled through the use of the Qt framework's \textit{QThread} API, and communication between threads utilises the framework's `\textit{signals} and \textit{slots}' feature, which allows components on different threads to send and receive data in a managed, thread-safe manner. The \textit{CameraController} class therefore utilises two signals; one for emitting the camera image data, and another for emitting the robot position data. At initialisation time the application's core class, \textit{MainWindow}, connects these signals to matching slots within the \textit{Visualiser} and \textit{DataModel} classes respectively.

The \textit{MachineVision} class handles the lower level operations related to the camera and the tracking system, including setting up the camera driver, retrieving and resizing individual images from the camera and running the ARuCo tag detection algorithm. The ARuCo software is included as an additional component of the OpenCV image processing library, and provides the function `\textit{aruco::detectMarkers}' which will run the tag detection algorithm on a given OpenCV image. For each tag detected in the image the ARuCo algorithm returns the pixel coordinates of the four corners of the tag. The \textit{MachineVision} class includes code to average these four coordinates, acquiring a central pixel coordinate, and then convert this to a 'proportional' coordinate; two numbers between 0 and 1 which represent the horizontal and vertical components of the position as a proportion of the full height and width of the image respectively. This ensures that the robot positions can still be correctly displayed after the image has been resized, without having to maintain information related to the resizing operation. The angle the robot is facing is also calculated. This is done by first calculating the position of the fourth corner, as a coordinate relative to the position of the third, and then applying an arctangent function to this coordinate. This angle is then converted to degrees and stored as an integer in order to reduce complexity, as a precision greater than one degree was not deemed necessary.

%----------------------------------------------------------------------------------------

\section{Networking}
The networking requirements of the system were relatively simple, and can be summarised as follows:

\begin{enumerate}
	\item Must utilize a WiFi network.
	\item Must allow a large number of sources to transmit data to a single host
	\item Must allow for frequent transmission of small packets of data
\end{enumerate}

WiFi networks utilise the standard Internet Protocol (IP) network layer protocol. There are two commonly supported transport layer protocols which run on top of IP, the Transmission Control Protocol (TCP) and the User Datagram Protocol (UDP). TCP is a managed and delivery-error checked protocol, and therefore guarantees that packets will be transmitted in the correct order, with lost packets being retransmitted. This adds overheads such as acknowledgements to the protocol, and requires an established 'connection' in order to function correctly. TCP also operates a queueing system, whereby packets for transmission are held until a number of them are ready, and can therefore be grouped together and sent. UDP, by contrast, does not error check the delivery of packets, making no guarantees that a packet will be received, or that packets will be received in the correct order, removing the need for an established connection and reducing the overheads involved. Packets can therefore be sent from any application to any target IP address and port on the network, without first establishing a connection with another application. Packets are also sent immediately, with no queueing system in place It was determined that the User Datagram Protocol (UDP) would be the most suitable for this system, for a number of reasons. The connection requirements of TCP would require the to form a connection with application each robot prior to transmitting data, which would add unnecessary complexity. Using UDP also ensured that the packets were transmitted immediately, reducing the potential for latency in the system. The lack of delivery checking was not considered an issue, as the robots would be transmitting updates frequently enough that a single lost packet would not cause a significant issue.

Code relating to the networking functionalities of the application is contained in the files \textit{datathread.cpp / .h}, which encapsulate the \textit{DataThread} class. This class is run on a dedicated thread to ensure that potentially blocking operations do not impact application performance. This class contains routines for dealing with the low level network requirements, such as establishing a socket through which to receive data packets from the robots, and continually listening on this socket for new data. All socket operations are implemented using structures and definitions from the standard C++ networking libraries. Data received from the robots is passed to the main application thread through a Qt signal, using the Qt signals and slots interface in the same manner as described in section \ref{VideoFeedAndTrackingSystem}. The main application class, \textit{MainWindow}, connects this signal to the appropriate slot in the \textit{DataModel} class. The application side networking is configured in the \textit{network} tab of the right-hand panel of the user interface. The user is able to enter a desired port number on which to receive data, and can begin listening for packets on this port by pressing the `\textit{start listening}' button.

Within the robot side API, the networking functionality is implemented in a very similar way. The initialisation function establishes a target socket based on a supplied IP address and port. This should match the IP address of the computer or server running the main application, and the port chosen by the user within the main application. When any of the functions for sending specific data packets are called, the data is then sent to this target socket. All socket operations are again implemented using the standard C++ networking libraries.

%----------------------------------------------------------------------------------------

\section{Data Transfer Format} \label{DataTransferFormat}
In order for the application to interpret and use data received from the robots, a common format for exchanging data needed to be defined, and used correctly at both ends of the communication link. A number of different options were considered for achieving this, ranging from super-lightweight custom packets using the minimum number of bytes, to established existing solutions such as the JSON data interchange standard. The primary concerns when making this decision were a desire to minimise any overhead in terms of extra code needed on the robot side, as the robots have limited memory, and to ensure the format remained as simple as possible, so that future extensions to the system, such as implementations for other robots, could be programmed with relative ease. It was ultimately decided not to use JSON, to avoid the need for any additional code libraries to be stored in the robot's memory, and to instead use a custom simple string-based packet format. All data to be transmitted from the robot to the application is therefore converted to a simple string which is then transmitted in the data packet. As well as containing the data, the string must identify the robot and describe the type of data within. The format for these strings is defined as three sections separated by space characters. The first section contains the numerical ID of the robot sending the packet. The second contains a number identifying the type of data contained in the packet. The last section contains the packet data, and has a variable format, depending on the packet's type. Figure \ref{fig:DataFormat} gives a visual representation of this format. Table \ref{tab:DataFormat} describes the purpose of each packet type, and describes the format of the `packet data' section for each.

\begin{figure}
	\centering
	\includegraphics[scale=0.3]{Figures/DataFormat.png}
	\decoRule
	\caption[Data Format]{The general format for each data packet.}
	\label{fig:DataFormat}
\end{figure}

\begin{longtable}{ l p{12cm} }
\caption[Data Format]{The format of the data section and the purpose of each packet type.}\\
 \hline
 \multicolumn{2}{p{12cm}}{\textbf{Watchdog Packet}}\\
 \hline
 Type ID & 0 \\
 Format & [ROBOT NAME]\\
 Purpose & Sent periodically to inform the application that the robot is still active. Also contains the robot's name, as should be displayed in the application.\\
 
 \hline
 \multicolumn{2}{p{12cm}}{\textbf{State}}\\
 \hline
 Type ID & 1 \\
 Format & [CURRENT STATE]\\
 Purpose & Informs the application of the robots current state.\\
 
 \hline
 \multicolumn{2}{p{12cm}}{\textbf{Position}}\\
 \hline
 Type ID & 2 \\
 Format & [X POSITION] \_ [Y POSITION] \_ [ANGLE]\\
 Purpose & Provides the application with a robot's position and orientation. This data is not sent by the robot, but instead comes from the tracking code.\\
 
 \hline
 \multicolumn{2}{p{12cm}}{\textbf{IR}}\\
 \hline
 Type ID & 3 \\
 Format & [SENSOR 1 DATA] \_ [SENSOR 2 DATA] \_ ... [SENSOR N DATA] \\
 Purpose & Contains a robot's infra-red sensor readings. Each sensor value is separated by a space, and the packet can contain as many values as the robot has IR sensors. \\
 
 \hline
 \multicolumn{2}{p{12cm}}{\textbf{Background IR}}\\
 \hline
 Type ID & 4 \\
 Format & [SENSOR 1 DATA] \_ [SENSOR 2 DATA] \_ ... [SENSOR N DATA] \\
 Purpose & Contains a robot's background infra-red sensor readings. Formatted the same as the standard IR data packet.\\
 
 \hline
 \multicolumn{2}{p{12cm}}{\textbf{Message}}\\
 \hline
 Type ID & 5 \\
 Format & [MESSAGE STRING]\\
 Purpose & This packet allows any general message to be sent from the robot to the application, and will be displayed in the application console and recorded in the logs. \\
 
 \hline
 \multicolumn{2}{p{12cm}}{\textbf{Custom Data}}\\
 \hline
 Type ID & 6 \\
 Format & [KEY] \_ [VALUE]\\
 Purpose & Contains a piece of custom user data, in the form of a key value pair. \\
	
 \label{tab:DataFormat}
\end{longtable}

%----------------------------------------------------------------------------------------

\section{User Interface} \label{UserInterfaceImplementation}
The user interface has been implemented using the Qt GUI framework. The layout for the interface is defined in \textit{mainwindow.ui}, a file generated by the Qt interface designer application to describe the structure and layout of the interface components in an XML based format. The Qt framework uses a standard event-driven interface paradigm, where events are generated when the user interacts with the interface, and code is used to define specific routines to execute in response to relevant events. The routines for handling interface events can be found in \textit{mainwindow.cpp / .h}, and are usually prefixed with the term `\textit{on\_}'. For example, the function `\textit{on\_actionExit\_triggered}' is called when the `Exit' action is selected from the menu, and instructs the application to close.

[INDIVIDUAL UI ELEMENTS w/ SCREENSHOTS]

In addition to the main user interface, a number of extra `dialog' windows are used to provide the user with access to the settings of each visualisation element, if settings are available. These dialog windows are defined in the files  \textit{idsettingsdialog.cpp / .h, pathsettingsdialog.cpp / .h} and \textit{proximitysettingsdialog.cpp / .h}. Dialog windows are a commonly used tool within application programming. They act as pop-up windows which usually require the user to either confirm or cancel some change. In this case the dialog windows present controls for changing specific visualisation settings, and the user can then either apply their changes or cancel them using the standard accept/reject buttons at the bottom of the window.

%----------------------------------------------------------------------------------------

\section{Visualiser}
The 'visualiser' is the name given to the custom user interface component that renders the augmented video feed. Implementing this component was key to satisfying the portion of the project aims related to augmented reality, and it forms one of the most visible elements of the system. The main visualiser component is defined in the \textit{Visualiser} class (\textit{visualiser.cpp / .h}), and a number of extra classes are used to define the associated settings and routines for visualising specific data types (\textit{VisConfig, VisID, VisName, VisState, VisPosition, VisDirection, VisProximity, VisPath}). Section \ref{VideoFeedAndTrackingSystem} describes the process of retrieving images from the camera and tracking the robots. The image data then arrives at the visualiser via a Qt slot function. At this stage the image is augmented based on the data in the data model, by iterating over the list of robots, and for each one iterating over the list of data visualisations, calling the render function for each. These render functions take the image and the current robot's data as arguments, and then add the relevant graphical representation to the image using the drawing functions within the OpenCV image processing library.

It was decided that an individual class would be implemented for each type of data visualisation, derived from the abstract class \textit{VisElement}. The aim was to make the visualisation process simpler to manage, and to follow object oriented practices, making it easier for new visualisation types to be added without modifying the underlying system. This also allows each data visualisation object to maintain its own settings, so that the visualiser component itself need not be aware of the details of the configuration of each data visualisation element. Instead each data visualisation element simply checks it's own configuration when it's render function is called.

The rest of the code in the \textit{Visualiser} class relates to embedding the OpenCV image within a Qt UI widget, and tertiary functionality such as detecting clicks within the image frame, and retrieving the frame's size. The OpenCV to Qt image conversion is done by instantiating a QImage instance directly from the internal pixel data of the OpenCv image. This QImage is then drawn onto the widget.

\subsection{Data Visualisations}
The visualiser supports a number of specific data visualisations:

\begin{enumerate}
 \item Displaying robot positions.
 \item Displaying robot 'orientations' by highlighting their forward directions.
 \item Displaying the ID of a robot as text close to its position.
 \item Displaying the name of a robot as text close to its position.
 \item Displaying the current state of a robot as text close to its position.
 \item Displaying a graphical representation of a robot's IR sensor data.
 \item Displaying the recent path a robot has taken as a line behind the robot.
\end{enumerate}

Each visualiser element can be enabled and disabled via the settings tab. Some of the visualisations have more complex settings, which can be accessed be double clicking the specific element in the visualisations list, also in the settings tab. Figure  shows the text based visualisations. Figure  shows the position and orientation visualisations. The IR sensor data visualisation is more complex, and offers a number of configuration options. The visualisation can either display in 'proximity mode' or 'heat mode'. In proximity mode lines are drawn in the direction each sensor is facing to approximate the proximity of the surface reflecting the IR pulses. This line therefore varies inversely with the value of the sensor. This mode can be seen in figure . In heat mode the IR sensors are represented as small blocks positioned around the perimeter of the robot. The size of the blocks and their colour varies in relation to their corresponding sensor value, as can be seen in figure . Both modes position their visualisations to correspond with the individual sensor positions, and this can be adjusted by the user via the IR visualisation settings dialog. The settings dialog also allows the user to decide whether IR data should be displayed for all the robots, which can overwhelm the display, or just the selected robot. The path visualisation can be seen in figure , and can also be configured to display for all robots or only the selected one. The user can also configure the interval between the position samples (as discussed in section \ref{DataModel}) via the settings dialog, which effects the smoothness and length of the path line.

[SCREENSHOTS]

%----------------------------------------------------------------------------------------

\section{Robot Side API} \label{RobotSide}
The robot side API is defined in `\textit{debug\_network.cpp / .h}' as a single class, \textit{DebugNetwork}. The contents of the \textit{DebugNetwork} class are described in table \ref{tab:RobotAPI}.

\begin{longtable}{ l l p{8cm} }
\caption[Robot API]{The contents of the \textit{DebugNetwork} class the forms the robot side API.}\\
 \hline
 \multicolumn{3}{l}{\textbf{Variables}} \\
 \hline
 Name & Type & Purpose\\
 \hline\\
 socket\_ready & Boolean & Indicates whether the socket has been created successfully, and it ready to be used.\\
 sock\_in & Struct & A structure defining the target socket.\\
 sock\_fd & Integer & Identifies the socket on the local machine (robot).\\
 robot\_id & Integer & The numerical ID of this robot. Set at initialisation time. Inserted to the header of each packet to identify the sender.\\
 \hline
 \multicolumn{3}{l}{\textbf{Functions}} \\
 \hline
 Name & Return Type & Purpose\\
 \hline\\
 init & Void & Initialises the class by setting up the socket and setting the ID for this robot. Requires a port number, a target host IP address and the robot ID as arguments. \\
 destroy & Void & Shuts down the socket. \\
 sendData & Void & Sends a given string as a raw data packet. \\
 sendWatchdogPacket & Void & Constructs and sends a watchdog packet. Requires the robot name to be provided as an argument. \\
 sendStatePacket & Void & Constructs and sends a state packet. Requires the current state to be provided as an argument string. \\
 sendIRDataPacket & Void & Constructs and sends an IR data packet. Requires the data be arranged into an array, and takes a pointer to the first element and a size value as parameters. Also takes an extra boolean parameter which can be set to true to indicate that this packet should be identified as backgroud IR data.\\
 sendLogMessage & Void & Constructs and sends a message packet to display a message in the application console and logs. The message string is supplied as an argument.\\
 sendCustomData & Void & Constructs and sends a custom data packet. The key value pair are supplied to this function as two argument strings.\\
 getRobotID & Integer & Returns the numerical robot ID, as currently set. \\
 setRobotID & Void & Sets the numerical robot ID based on an integer argument. \\
 
 \label{tab:RobotAPI}
\end{longtable}

In order to utilize this API a user can modify their robot controller code to include the \textit{debug\_network.h} header file, call the \textit{init} function at start-up time, and then call the various packet and data functions whenever necessary. The user is therefore in charge of how frequently data is transmitted, and can decide whether to simply send updates to the application when the robot's data changes, or to transmit a fixed quantity of data every control step. It was potentially possible for the API to handle all data reporting in the background, without requiring the user to specify when to transmit data in the controller code, however this approach was not taken for a number of reasons. Firstly it limits the control and flexibility available to the user. Each swarm system is different, and may have different requirements, hence the API should allow the developer to make decisions regarding data reporting which are right for their specific case. It would also limit the portability of the API, as a more automatic system would likely have to hook into the lower level robot drivers, meaning much greater changes would be necessary to port the code to a different robot. Furthermore by allowing the user to have complete control over when and how frequently data is transmitted they are able to manage the amount of traffic they are putting on their network, potentially mitigating congestion issues with very large swarms. Finally allowing users to control the moments within the code when data is reported to the debugging system was deemed to be a more intuitive system, especially as many developers are already familiar with the concept of using `print' statements within code to display debugging messages in a console.

%----------------------------------------------------------------------------------------