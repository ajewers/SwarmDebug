% Chapter 2

\chapter[Literature Review]{Literature Review} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

\section{Overview}
This section presents a review of some of the literature from the field of swarm robotics, including some general summaries of the field's fundamentals, as well as a number of specific pieces of swarm robotics research with particular relevance to this project. It also presents a number of pieces of research from other, related areas, such as human-robot interaction and robotics-focused augmented reality research. The results of this literature survey informed the project direction significantly, and formed the basis for many of the design and implementation decisions made later on. It is also presented with the aim of providing the reader with the base of knowledge required to better understand the project. The literature covered in this section can be separated into several broad topics, each informing a different element of the project work.

Firstly an understanding of the fundamental concepts of Swarm Robotics, and to a lesser extent Swarm Intelligence, was deemed key to producing an application that is useful in practice, and will help a reader to better understand the purpose and aims of the project. A number of key publications related to the core concepts of swarm robotics are presented in Section \ref{GeneralSwarmRobotics}. A deep understanding of the technical details of specific swarm systems, such as specific behavioural algorithms or implementation details, is not a priority for understanding this project, as the application aims to be more broadly applicable to a wide range of swarm systems. Emphasis has instead been placed on understanding the general classification of swarm robotic systems, relevant problem domains, and recurring concepts, so that the system might better serve researchers in the field.

This project focuses on a piece of software which forms an interface between a human operator and a robot swarm. A relevant area of research is therefore Human-Swarm Interaction (HSI). Research in this area focuses on the different ways in which humans and robot swarms can interact, the different roles humans take whilst interacting with robot swarms, and the best practices for facilitating this interaction given different aims, and different user roles (developer, researcher, end user, etc.). The two key challenges of HSI are control - how best to allow a human operator to direct the behaviour of a decentralised swarm - and monitoring - how to retrieve data from a swarm and present it in a useful, human readable manner. This project is primarily related to debugging robot swarm behaviours, and therefore the literature presented from this area focuses on the monitoring side of HSI. An overview of the relevant Human-Swarm Interaction literature is presented in Section \ref{HumanSwarmInteraction}.

As previously explained, augmented reality technologies have been identified as a potentially powerful tool for facilitating human-robot interaction and communication. Section \ref{AugmentedReality} reviews a number of pieces of research relating to the use of AR technologies in conjunction with robotic systems.

A number of pieces of research have investigated different ways to monitor and debug single robot and multi-robot systems, resulting in several systems that use a range of the technologies and concepts discussed previously. These systems share areas of common ground with this project. These works are summarised and reviewed in section \ref{SimilarWork}, with a specific focus on other systems applying graphical, AR-style techniques to a debugging context.

%----------------------------------------------------------------------------------------

\section{Swarm Intelligence and Swarm Robotics} \label{GeneralSwarmRobotics}
Sahin \cite{Sahin:2004} presents a summary of the key concepts of swarm robotics, and attempts to offer a coherent description of the topic. He notes that a key difference from other multi-robot systems is the lack of centralised control, and the idea that desired behaviour should emerge from simple local interactions between robots, and between the robots and their environment. He also notes some of the key motivators behind Swarm Robotics research, stating that a swarm robotics system would ideally exhibit ``\textit{robustness}'', ``\textit{flexibility}'' and ``\textit{scalability}'' \cite{Sahin:2004}. Robustness refers to the swarm's ability to continue to function should one or more individual swarm members suffer a failure of some kind. Flexibility refers to the swarm's ability to adapt to changes in the environment without the need for re-programming. Scalability describes the idea that a swarm should be functional at a range of sizes, and that ideally the number of robots in the swarm could be increased or decreased depending on the demands of the task. These descriptors should be taken into account by the design of the system presented in this project. In order for the system to be able to work well with robust, scalable swarms it should adapt easily to variation in the number of robots being monitored. Newly detected robots should therefore be incorporated seamlessly. The video feed component of the system will allow a user to see the environment the swarm is interacting with and observe how robots respond to changes within it, and therefore judge the extent to which the swarm exhibits flexibility.

Sahin \cite{Sahin:2004} goes on to describe several classes of application for which Swarm Robotics systems might be well suited. Tasks that cover a region could benefit from a swarm's ability to distribute physically in a space according to need. Dangerous tasks could benefit from the relative dispensability of individual robots in the swarm; should one be damaged or destroyed the swarm could continue to function, and it would be less costly that the loss of a single, complex, expensive robot. Tasks requiring scalability are good candidates, as discussed before, and tasks that require redundancy are also highlighted, as swarm systems should have the ability to degrade gracefully, rather than suffering a single catastrophic failure. Through this generalisation of the application areas, insight can be gained into the kinds of work swarm robotics researchers are likely to be doing, and this should inform the design of the application. Overall Sahin's paper \cite{Sahin:2004} provides a coherent, succinct overview of the field, and although it is now over a decade old the concepts covered remain relevant.

%The paper also contains a wealth of further reading, including papers on developing specifc behavioural paradigms such as self-organisation \cite{SelfOrganizing} and path-formation \cite{PathFormation}, which give insight into the kinds of information and data that swarm robots use to make decisions, and that a swarm researcher might therefore be interested in monitoring. Beni \cite{FromSIToSR} presents a relatively informal but useful overview of the terminology used in the field, which may serve as useful additional reading to Sahin's overview.

The book `\textit{Swarm Intelligence: From Natural to Artificial Systems}', by Bonabeau, Dorigo and Theraulaz \cite{Bonabeau:1999}, provides in its introductory chapter a good overview of the biological concepts and animal behaviours which inspired much of the research that led to the creation of the field of swarm intelligence. Fundamental concepts such as self-organisation and decentralised control are discussed. In order to be a useful tool in the swarm robotics research space, it is important that the system developed during this project does not violate these core principles of the swarm robotics paradigm. Therefore the system should not facilitate low level control of the robots or allow for forms of communication and data exchange that might invalidate the self-organising, decentralised nature of the swarm's behaviour. The later chapters \cite{Bonabeau:1999} provide a detailed look at several key insect behaviours, and how mathematical models and algorithms can be derived to mimic them. An understanding of these behaviours and models can offer insight into what information the application might need to expose to allow a user to validate the correct operation of a swarm behaviour based on these concepts. A number of these algorithms focus or rely on data related to the position and movement of individuals within the swarm, and the swarm as a whole. The system developed during this project should reflect the importance of this data and provide features which aid the user in interpreting it.

In a more modern work, Brambilla et al. \cite{Brambilla:2013} focus heavily on the engineering practicalities of designing, implementing and testing swarm robotic systems. The authors then apply this focus as a means by which to classify and critique a large body of swarm robotics research, noting that although much work has been carried out regarding the design and analysis of swarm behaviours, other areas including maintenance and performance measurement are heavily lacking in research contributions. Debugging can be considered a maintenance task, and the system developed in this project has potential in both maintenance and performance measurement applications. It is hoped therefore that this work might contribute to this area of the field, by investigating through implementation the practicalities of a generalised swarm maintenance and observation software application. The authors \cite{Brambilla:2013} later note whilst concluding that human-swarm interaction remains an open issue, and will be key to realising functional, real-world swarm robotic systems. They identify a number of works related to HSI, almost all of which focus on the task of controlling a robot swarm, and involve different methods by which a user might insert data into the swarm system. Little consideration appears to be given, both in this paper and throughout the literature, to the task of monitoring a swarm, and best practices for retrieving information in a manner that is useful to a human operator.

%----------------------------------------------------------------------------------------

\section{Human Swarm Interaction} \label{HumanSwarmInteraction}
In their paper `\textit{Human Interaction with Robot Swarms: A Survey}' \cite{Kolling:2016} Kolling at el. begin by noting the lack of research into methods for interfacing humans and robot swarms. They suggest that real-world applications for swarm robotics systems are now within reach, and that discovering effective methods for allowing humans to control and/or supervise swarms is a key barrier to realising these systems. The paper \cite{Kolling:2016} provides a detailed analysis of human swarm interaction from a number of different perspectives. Of relevance to this project is the statement on page 15 that ``\textit{Proper supervision of a semiautonomous swarm requires the human operator to be able to observe the state and motion of the swarm, as well as predict its future state to within some reasonable accuracy}'' \cite{Kolling:2016}. This statement lends credence to the aims of this project, as swarm supervision and swarm debugging are highly comparable tasks; both involve observing the swarm whilst performing its task and determining the validity of the behaviour observed. The system developed in this project should allow the state of the swarm, including the internal state of individual robots, to be observed simultaneously with the physical positions and motions of the robots within their environment. The paper \cite{Kolling:2016} goes on to suggest that by observing the swarm over time the human operator will be able to provide `\textit{appropriate control inputs}'. In the case of this application, rather than providing control input, the human operator will be seeking to identify faults, and provide appropriate corrections to the system, however the concept of state visualisation remains relevant.

Rule and Forlizzi \cite{Rule:2012} present a thorough examination of the complexities of human robot interaction (HRI) when dealing with multi-robot (and multi-user) systems. Much of the paper focusses on control methods, which are not directly applicable to this project, however section 2.4 titled \textit{Salience of Information} discusses the task of designing interfaces for displaying information about multi-robot systems to a human operator in a manner which is both information dense and rapidly understandable. The authors note that the use of colour has been shown to improve interface readability \cite{Christ:1984}, and that the brain has been shown to process text faster than images \cite{Carney:1998}, hence complex icons should be avoided. These ideas should be incorporated into the design of the application user interface for this project. A range of different designs could be explored, including finding a balance between the amount of information displayed graphically, and the amount displayed textually, and deciding whether to use colour to differentiate between individual robots, or to differentiate between different types of data, or a combination of both.

The authors \cite{Rule:2012} then go on to use `\textit{contextual inquiry}' interviews with robot operators to determine a set of questions which, if answered, should allow for robust robot operation. Of relevance to this project are the four questions in the \textit{human-robot} category. These are 1) ``\textit{What mode, state, and environment is the robot in and how will this affect my commands?}'', 2) ``\textit{Which robot needs my attention?}'', 3) ``\textit{What is each robot accomplishing?}'', 4) ``\textit{What can I accomplish with this robot?}''. Providing the user with answers to questions two, three and four is beyond the scope of this project, however the system should provide the user with an answer to question one, by allowing access to state and environment information simultaneously. Once again although the authors \cite{Rule:2012} consider only a robot control perspective, these questions remain broadly relevant to a debugging perspective also. For example, question one could be rephrased as \textit{What mode, state, and environment is the robot in, and is this having the correct effect on it's behaviour?} to better fit the use case of this project. When discussing their results, specifically the appropriate level of display salience for robot state awareness in section 4.8 \cite{Rule:2012}, the authors note that users wanted the ability to ``\textit{select which aspects of robot information to view at any one time}''. They also state that users wanted basic overview information, with the ability to access a more detailed view specific to one robot when ``troubleshooting''. This configurable, layered approach to information display and access should be incorporated into the user interface design for the system.

The authors \cite{Rule:2012} do not state how they determined that the set of ``\textit{essential situation awareness questions}'' presented ensured robustness. The contextual inquiry method is potentially subjective, suggesting that answering only these questions may not guarantee full awareness. Other factors which a system user could benefit from an awareness of should be considered. For example the authors mention the robot's mode, state and environment, but do not include the robot's sensor data. Furthermore care must be taken when applying the results of this work to swarm robotic systems, as it was not produced with swarms specifically in mind. Therefore applying the results within, without also considering needs specific to swarm robotics, could lead to too strong a focus on the actions of individual robots, and a lack of focus on the overall behaviour of the swarm.

%----------------------------------------------------------------------------------------

\section{Robotics Debugging} \label{RoboticsDebugging}
Collet and MacDonald \cite{Collet:2006} describe in detail the difficulties in debugging robotics systems. The authors identify that the difficulties in developing and debugging robotics applications when compared to traditional software arise from either the environment of the robot - which will often be ``\textit{uncontrolled}'' and ``\textit{dynamic}'' - or from the mobile nature of the robot. Because the environment a given robot operates in is a real world space, the level of control that can be exerted over it by the researcher or operator is inherently limited \cite{Collet:2006}. The environment may therefore change over time, exhibit imperfections, and include other time-varying elements. A robot is a physical actor and will likely experience dynamic change in its sensor readings and its relationship to the environment over time. This is especially true for mobile robots, whose position and orientation will change over time. The behaviour of the robot often largely depends on these highly variable factors, and therefore replicating a given behaviour exactly becomes almost impossible. The authors go on to state that difficulties in debugging often arise from ``\textit{the programmer's lack of understanding of the robot's world view} \cite{Collet:2006}''. It can therefore be extrapolated that for a multi robot system such as a robot swarm this problem would be exacerbated. Each robot will have its own perception of the environment, which will differ based on differences in the robots positions and orientations as well as variations in the instrumentation of each robot. For a multi-robot system the programmer is required to have an understanding of not just one but multiple world views, adding yet more potential for error and inaccuracy, and further obscuring bugs or behavioural issues that the programmer is trying to diagnose. This work \cite{Collet:2006} suggests that developers will need specific tools which enhance their understanding of the robots' world view in order to develop effectively for swarm robotic platforms. This need forms the mandate for the majority this project. Collet and MacDonald \cite{Collet:2006} go on to discuss the use of augmented reality to satisfy this need, and present an augmented reality based software tool for this purpose, which is discussed in Section \ref{AugmentedReality}.

Gumbley and MacDonald \cite{Gumbley:2009} identify one of the core issues in debugging robotic software using traditional debugging software to be the assumption of a ``deterministic, suspendable environment''. This assumption rarely holds true for robots, due to their existence in a real world environment. The authors note that traditional debugging concepts such as breakpoints pause code execution, but cannot for obvious reasons also pause a robot's environment. This allows the robots environment to change whilst execution is paused, and therefore affecting the robot's behaviour. In the case where a breakpoint has been inserted to try to isolate the cause of a previously observed fault, this change in behaviour may also stop the fault from occurring. The authors \cite{Gumbley:2009} go on to consider a number of possible methods by which a developer can obtain information without pausing execution, including live data extraction which is the method chosen in this project. They note that adding code to extract information without pausing execution has the potential to affect robot behaviour. This is a salient point, and should be considered when implementing the robot-side portion of this project, where care must be taken to minimize the affect data reporting has on the execution of the robot's actual behaviour. Issues of this nature could be caused by the data reporting code taking too long to execute, thus disrupting robot behaviour. Keeping the data reporting code lightweight and efficient should therefore be a priority.

%----------------------------------------------------------------------------------------

\section{AR and Robotics} \label{AugmentedReality}
Augmented reality presents a powerful tool for use with robotics, and specifically for debugging robotics, as it allows information gathered by a robot about an environment to be superimposed onto that environment in a way which can be inherently understood by humans. Milgram et al. \cite{Milgram:1993} discuss the different communication formats used to interface between humans and robots, grouping them into ``\textit{continuous}'' and ``\textit{discrete}'' formats. For any communication involving a spatial or temporal component, the process of converting to and from a discrete format in order to transmit this information is an unnecessary burden. Both humans and robots use the continuous spatial dimensions, and humans have an inherent, instinctive understanding of physical things expressed in three dimensions. The authors \cite{Milgram:1993} therefore identify that  augmented reality provides an excellent means of supporting the communication of spatial information. Their paper focuses on the combination of stereoscopic displays and computer generated graphics to allow for more intuitive control of robotic systems. In the case of this project the concept is reversed; robots reporting spatial information for validation by a user should do so in a format which is inherently continuous such as a graphical visualisation in AR, rather than one that is discrete such as text-based numerical output. This should in theory reduce the time required for a human to process the information. The authors note \cite{Milgram:1993} that the ideal system utilises both discrete and continuous formats where appropriate to best communicate the required information, and is ergonomically designed to allow the user to make use of both easily and intuitively.

Like much of the literature surveyed, this paper \cite{Milgram:1993} is focused primarily on robot control, rather than observation and monitoring. In spite of this much of the content of the paper remains applicable. Since the paper was written, just over twenty five years ago, major advancements have been made in virtually every area mentioned, including the quality and precision of robotic systems, their cognitive, perceptive and decision making abilities, augmented reality technologies and robotic autonomy. Because of this, some of the contents of the paper have fallen out of date. Specifically, the assumption that robots lack the level of autonomy required to survey their environment and then form and execute a series of steps to carry out a relatively high level task, such as ``find and go to object Q''\cite{Milgram:1993}, is no longer necessarily true. A number of modern robots possess sufficient sensing capabilities, processing power and cognitive programming to perform such tasks based on high level commands. This does not however de-value the AR methods discussed within, and given the increased complexity and sensing capabilities of modern robots, AR based methods of interaction actually have more potential than ever. The paper however makes no mention of the potential for an AR system to report data from a robot's sensors visually, which may be attributed to the technological limitations of the time rather than an oversight by the authors.

[HEAD MOUNTED AR w/ ROBOTS]

Collet and MacDonald \cite{Collet:2006} suggest that augmented reality tools can address and mitigate some of the robotics debugging issues discussed in section \ref{RoboticsDebugging} by superimposing graphical representations of the robot's understanding of the environment on top of a live view of the environment itself \cite{Collet:2006}. Hence the programmer is able to see how the robot has interpreted the environment, and identify inconsistencies. The authors describe the image of the real world environment as the ``\textit{ground truth}'' against which the robot's view can be compared and contrasted \cite{Collet:2006}. Figure \ref{fig:Sonar} shows a visual example of this technique, where the data received from the robot's sonar sensors is converted to spatially situated 3D shapes and superimposed over the live image, and can therefore be verified visually by the user.

\begin{figure}
	\begin{center}
	\includegraphics[scale=0.6]{Sonar.png}
	\decoRule
	\caption[Sonar data visualisation. Collet and MacDonald \cite{Collet:2006}]{Sonar sensor data visualisation in Collet and MacDonald's system \cite{Collet:2006}.}
	\label{fig:Sonar}
	\end{center}
\end{figure}

The application developed during this project closely follows this paradigm; allowing the user to identify bugs by comparing the robot's knowledge of its environment and its decision making factors (collectively referred to as its state) with a view of the environment, in real time. The application aims to apply this concept specifically to swarm robotics systems, and therefore must allow the user to compare the states of multiple robots with the environment simultaneously. From the perspective of each robot in the swarm, the other robots will form part of the environment, therefore the application must take this into account when displaying the information. Because of the large increase in information from a single-robot system to a multi-robot one, it becomes important that the application provide a way for the user to filter what information is displayed, allowing them to focus on the primary aspect under test. Filtering also allows the user to compare and contrast specific robots against one another by filtering out information related to other robots, or by displaying in more detail information related to the robots of interest.

\section{Similar Work} \label{SimilarWork}
Ghiringhelli et al. \cite{Ghiringhelli:2014} present a system for augmenting a video feed of an environment containing a number of robots with real time information obtained from each of the robots. This is similar in concept to the system described by Collet and MacDonald \cite{Collet:2006}, but is designed specifically to target a multi-robot system. The authors identify the ability to overlay spatial information exposed by the robots on to the video feed in real time, in the form of situated graphical representations, as the most important debugging feature of the system. Figure \ref{fig:SpatiallySituated} shows a spatially situated overlay of data exposed by robot thirty two, in the authors system \cite{Ghiringhelli:2014}. A viewer is able to immediately verify the validity of the robot's world view from this image by comparing the blue overlay to the image beneath. Each robot features a coloured LED blinking a unique coded pattern to enable tracking, and the system uses homography techniques to map between the robots' frame of reference and the camera's. The project proposed in this report intends to use a simpler approach, with position and orientation tracking achieved through the use of the AruCo \cite{Garrido:2014} marker-based tracking system, and a birds-eye view position for the camera to simplify mapping by effectively reducing the space to a 2D approximation.

\begin{figure}
	\begin{center}
	\includegraphics[scale=0.8]{SpatiallySituatedData.png}
	\decoRule
	\caption[Spatially situation data overlay. Garrido et al. \cite{Garrido:2014}]{Example of spatially situated data overlayed on a live image in \cite{Garrido:2014}.}
	\label{fig:SpatiallySituated}
	\end{center}
\end{figure}
