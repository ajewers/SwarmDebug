% Evaluation Chapter

\chapter[Evaluation]{Evaluation} % Main chapter title

\label{ChapterEvaluation} % For referencing the chapter elsewhere, use \ref{Chapter10} 

This chapter contains an evaluation of the swarm debugging system. The first portion of this evaluation is derived from the results of a number of `\textit{user evaluation sessions}' which are discussed in section \ref{UserEvaluationSessions}, and aim to determine the extent to which the system is useful in its intended context. The second portion is based on a comparison of the system with the aim and objectives of the project, as stated in section \ref{AimAndObjectives}.

%----------------------------------------------------------------------------------------

\section{User Evaluation Sessions} \label{UserEvaluationSessions}

Software testing can be used to verify that a system works as intended, however this does not guarantee that it is useful in practice. The purpose of the user evaluation sessions was therefore to determine whether or not the system satisfied its intended purpose. In order to do this, potential system users were asked to use the system to complete a number of tasks, and data was recorded regarding their experience. This data was obtained through direct observation of the participants interaction with the system, and through a follow up questionnaire designed to gauge their opinions on the system's various features and functionality. One of the stated objectives of the project was to build a system with a clear and intuitive user interface, hence part of the purpose of the user evaluation sessions was to determine whether or not the user interface was intuitive to use and easy to navigate, without prior knowledge. It was therefore necessary to select participants who had no direct involvement in the development of the system, and were therefore not biased by an existing understanding of the user interface.

The user evaluation sessions followed an observed testing format. Participants were given full control of the system, which was initialised to a known state, and asked to complete a number of tasks, ranging from simple information and data location and retrieval to a full simulated debugging task. Data was collected by taking notes on how the participants interacted with the system, including any difficulties they had, and any comments they made whilst using it. The primary areas of interest for these notes were how easily the participant was able to navigate the user interface and obtain information, how often they asked for additional information, and what they asked for information regarding, as well as if (and how quickly) they were able to isolate and correct the deliberate behavioural fault during the debugging task, and what features of the system they made use of during their attempt.

\subsection{Set Up}
Four robots were used during the sessions, each loaded with the same behaviours. Two simple behaviours were programmed for the robots prior to the start of the sessions. The first was a very simple data test behaviour which had the robot drive slowly in a circular path, whilst reporting data for all of the types defined in the system. This included a watchdog packet every ten control steps containing a unique name for each robot, a varying state which would change every fifty control steps between three possible values, active and background IR data, an arbitrary log message packet every hundred control steps and a custom data packet every ten control steps, containing the current total number of control steps as its data. 

The second behaviour was a simple dispersion behaviour, which would cause the robot to turn and drive away from any nearby object. This was achieved by monitoring the values of the robots IR sensors, calculating a vector of highest IR reflection intensity based on these values, and negating it to acquire a heading vector in the opposite direction. Sensor values were only included in the reflection intensity vector calculation if they surpassed a specific threshold, in an attempt to eliminate the effects of random noise. Whilst executing this behaviour the robot was also reporting debugging information to the application. This included watchdog packets, active and passive IR data, and the control step as custom data all as before, as well as the state of the robot which would vary between \textit{IDLE} and \textit{MOVING}. In order to simulate a bug in the robot's behaviour code, the IR value threshold in this second behaviour was deliberately set too low, causing the robots to move randomly when no objects were nearby, as a result of the fluctuations in the IR readings due to noise. A further consequence of this was that the robot would rarely, if ever, enter the \textit{IDLE} state. Participants were then asked to attempt to locate the cause of this bug using the debugging system.

Prior to the start of each session the robots were returned to their initial state, with the two behaviours loaded and the first behaviour running. The application was not set up in any way, and was run fresh for each session. It was important that this starting set up be the same for each participant, in order for the results to be comparable.

\subsection{Session Sequence}
The following sequence of steps was then carried out for each session. Participants were deliberately not told precisely where to find specific information, or exactly how to complete each task, in order to observe the extent to which the user interface was intuitive. Participants were asked to say if they could not determine how to complete one of the tasks, at which point further guidance was given. Relevant observational notes were taken at each step.

\begin{enumerate}
 \item Introduce the application to the participant, and have them run the executable.
 \item Explain the purpose of the application, and the purpose of this evaluation session.
 \item Indicate the key features within the application. Specifically identify the visualiser and explain its purpose, as well as the robot list panel and the data panel.
 \item Ask the participant to start the application listening for data packets on the network. Mention that this functionality can be found on the network tab only if necessary.
 \item The simple circular motion controller code should be running on the robots already. If it is not, run it now.
 \item Ask the participant to obtain the following pieces of information:
 \begin{enumerate}
  \item The current state of robot 1
  \item The recent state changes of robot 2
  \item The current IR sensor values of robot 3
  \item The current value of the `ControlStep' custom data point for robot 4
  \item The set of known states for robot 1
  \item The numerical angle describing the orientation of robot 2
 \end{enumerate}
 \item Ask the participant to use the visualiser settings tab to achieve the following:
 \begin{enumerate}
  \item Hide the name and state visualisation for all robots
  \item Display the recent path for all robots
  \item Display the IR sensor data for the selected robot in heat mode
  \item Hide the position and orientation visualisation for all robots
  \item Display the custom data point `ControlStep' for the selected robot.
 \end{enumerate}
 \item Ask the participant to use the camera settings tab to add a mapping from ARuCo tag ID 2 to robot ID 10. Explain further if necessary.
 \item Ask the participant to use the logging tab to achieve the following:
 \begin{enumerate}
  \item Set the logging directory to a specific folder
  \item Start logging for a short period of time and then stop it.
 \end{enumerate}
 \item Stop the circular motion behaviour on the robots and switch to the dispersion behaviour.
 \item Make the dispersion behaviour source code available to the user.
 \item Explain the desired behaviour to the participant, and the current erroneous behaviour.
 \item Ask the participant to attempt to locate the cause of the issue using the data made available by the debugging system. Provide hints and advice only if necessary.
\end{enumerate}

\subsection{Questionnaire}
Following the practical session, participants were asked to complete a questionnaire regarding their experience, and their opinions of the system and its features. The questionnaire included the following questions:

\noindent\textbf{Question 1: How would you rate your overall impression of the user interface?}\\(Scale of 1 to 5, poor to excellent)

\begin{center}
\begin{tabular}{ l c c c c c c }
 Group & 1 & 2 & 3 & 4 & 5 & Average \\ 
 \hline
 Domain Experts & x & x & x & x & x & a \\
 Others 		& x & x & x & x & x & a \\
\end{tabular}
\end{center}

\noindent\textbf{Question 2: }\\(Scale of 1 to 5, )

\begin{center}
\begin{tabular}{ l c c c c c c }
 Group & 1 & 2 & 3 & 4 & 5 & Average \\ 
 \hline
 Domain Experts & x & x & x & x & x & a \\
 Others 		& x & x & x & x & x & a \\
\end{tabular}
\end{center}

\noindent\textbf{Question 3: }\\(Scale of 1 to 5, )

\begin{center}
\begin{tabular}{ l c c c c c c }
 Group & 1 & 2 & 3 & 4 & 5 & Average \\ 
 \hline
 Domain Experts & x & x & x & x & x & a \\
 Others 		& x & x & x & x & x & a \\
\end{tabular}
\end{center}

\noindent\textbf{Question 4: }\\(Scale of 1 to 5, )

\begin{center}
\begin{tabular}{ l c c c c c c }
 Group & 1 & 2 & 3 & 4 & 5 & Average \\ 
 \hline
 Domain Experts & x & x & x & x & x & a \\
 Others 		& x & x & x & x & x & a \\
\end{tabular}
\end{center}

\noindent\textbf{Question 5: }\\(Scale of 1 to 5, )

\begin{center}
\begin{tabular}{ l c c c c c c }
 Group & 1 & 2 & 3 & 4 & 5 & Average \\ 
 \hline
 Domain Experts & x & x & x & x & x & a \\
 Others 		& x & x & x & x & x & a \\
\end{tabular}
\end{center}

\noindent\textbf{Question 6: }\\(Scale of 1 to 5, )

\begin{center}
\begin{tabular}{ l c c c c c c }
 Group & 1 & 2 & 3 & 4 & 5 & Average \\ 
 \hline
 Domain Experts & x & x & x & x & x & a \\
 Others 		& x & x & x & x & x & a \\
\end{tabular}
\end{center}

\noindent\textbf{Question 7: }\\(Scale of 1 to 5, )

\begin{center}
\begin{tabular}{ l c c c c c c }
 Group & 1 & 2 & 3 & 4 & 5 & Average \\ 
 \hline
 Domain Experts & x & x & x & x & x & a \\
 Others 		& x & x & x & x & x & a \\
\end{tabular}
\end{center}

\noindent\textbf{Question 8: }\\(Scale of 1 to 5, )

\begin{center}
\begin{tabular}{ l c c c c c c }
 Group & 1 & 2 & 3 & 4 & 5 & Average \\ 
 \hline
 Domain Experts & x & x & x & x & x & a \\
 Others 		& x & x & x & x & x & a \\
\end{tabular}
\end{center}

%----------------------------------------------------------------------------------------

\subsection{Participants}

Two groups of participants took part in the user evaluation sessions. The first was composed of `\textit{domain experts}'; researchers and other technical people with experience in the field of swarm robotics. This group were more likely to have an understanding of the needs of the system's target users, and therefore their input was considered more valuable. However because of the relatively niche nature of the field, the availability of participants in this category was limited. Hence only a small number could be found to participate in the sessions.

In order to remedy this lack of participants, the second group was introduced. This group was composed of undergraduate engineering students, who had a basic understanding of the core concepts of swarm robotics, but little or no practical experience. Where necessary an introduction to the core concepts was given before the session. This group did however possess general knowledge regarding software development, as this was necessary to complete the sessions effectively. The results obtained from this group were therefore mostly useful to evaluate elements of the system unrelated to swarm robotics, such as the usability of the user interface and the clarity of information display. This difference is reflected in the analysis of the results.

%----------------------------------------------------------------------------------------

\subsection{Results}




%----------------------------------------------------------------------------------------

\subsection{Analysis}




%----------------------------------------------------------------------------------------

\section{Comparison with Project Aim and Objectives}




%----------------------------------------------------------------------------------------

\section{System Limitations}

In spite of the projects success in achieving the aims and producing a working system, a number of limitations inherent to the systems nature have been identified and are summarised here.

\begin{description}
 \item [Effect on robot behaviour.] In order to receive data from the robots, the system requires that   additions are made to the robot's controller code to report this data. Any time code is added to the robot's controller it has the potential to effect the behaviour in unforeseen ways, even if the added code has no direct effect on the code controlling decision making. This is because time is required to execute the data reporting code, and this can have a knock on effect on the execution of the rest of the code. This is most dangerous when deploying a swarm system, as the developer may choose to remove all of the data reporting code prior to producing the `release' version of the system, as this code is debugging focused, and not needed in a final version once correct operation has been established. If there is a significant amount of data reporting code, removing it all could have a non-negligible effect on the robots behaviour, leading to potential unpredictability. In order to mitigate this problem as much as possible the robot side code has been implemented to require as little time as possible to transmit data packets.
 
\item [WiFi network requirement.] The system requires that all robots be connected to one WiFi network in order to operate correctly. This means the current system is limited to robots which can support WiFi, and requires that the network infrastructure be in place. Considering the system's intention as a debugging tool, and therefore its likely use within laboratories, WiFi network infrastructure is not an unreasonable requirement. However not all robot platforms support WiFi, hence this system in its current state is limited to robot platforms that do. Section \ref{HardwareExpansion} discusses extending the system in future to support a further wireless protocol such as Bluetooth.

\item [Fixed Camera Viewpoint.] The video based portion of the system has been implemented to function with the video captured from a fixed viewpoint, with an overhead, birds-eye view of the robots. The system cannot correctly augment the video if it is captured from any other angle, even though the tag detection algorithm can cope with a range of angles. The system is therefore limited to consider and augment the space in only be two dimensions, and cannot display height-related data in any way. Many consider a three-dimensional reference frame to be a requirement of a true augmented-reality system \cite{Azuma:1997}, cite{Billinghurst:2014}, and this system would have to be modified to support a non-specific viewpoint in order to work with modern augmented reality hardware.
\end{description}
