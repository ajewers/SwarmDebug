Project Presentation Script

1: Title Slide
  Good afternoon, my name is Ali Jewers, and the project I am working on is titled 'An Augmented REality Debugging System for Robot Swarms', and my supervisor is Alan Millard.
  
2: Introduction - SR
  This project largely falls into the domain of Swarm Robotics, which is a relatively new area of research, focusing on a specific type of multi-robot system. The general concept is to use a large number of fairly simple robots to solve a task cooperatively. One of the main characteristics of a robot swarm as opposed to a regular multi-robot system is the lack of centralised control. The robots should enact a simple behaviour based on the information available to them locally, and without being directly instructed by some more knowledgeable control unit. This field of research originated from studies into the behaviour of social insects, and attmepts to recreate the observed behaviours in robots. The field has been gaining momentum in the last ten years due to the decreasing costs and size requirements of computing power and the increasing quality of small robotic components. Swarm robotics presents a fairly unique challenge for the researchers and developers creating swarm behaviours, as swarms can be very difficult to debug, due to the large number of robots involved, and the lack of a central control point to gather information from.
  
3: Introduction - Debugging Tools
  When considering implementing robotic behaviours as a programming task, it becomes apparent that there are some key differences. These largely stem from the fact that robots operate in a real-world environment rather than the digital environment of traditional software. This means there are a much greater range of inputs and factors that might affect the robot's decision making and overall behaviour. A lot of these inputs are fully continuous, rather than the discrete inputs commonly dealt with in the digital domain. This can make it almost impossible to exactly reproduce any observed fault, as there can be an infinite number of configurations for the influencing factors. It also leads to bugs being caused simply by the 'noise' inherent in a real world environment - the slight random variation in any sensor measurement or actuator movement. This problem is therefore amplified when debugging a swarm robotics system, as incresing the number of robots multiplies the number of these influencing factors. Traditional debugging tools, which are often text based, and require execution to be paused to retrieve information, are therefore not always sufficient when working in this space. New tools need to be created which can retrieve and display information from an active robot swarm in real time, in a manner which can be easily and quickly understood by a human operator.
  
4: Introduction - Augmented Reality
  Currently emerging augmented reality technologies present new opportunities for working with robots, and may pose a solution to the problem illustrated before. An augmented space combines a real space with a digital one, and creates a hybrid reality which can be directly understood and influenced by both humans and robots. Humans have an innate understanding of three dimensional spaces, and can parse information from these much faster than from text or numbers. Allowing humans and robots to share a mixed reality therefore has the potential to significatly broaden the human-robot communication channel.
  
5: Introduction - Project Concept
  Bearing in mind the three elements I've just discussed, this project aims to develop a software tool for monitoring and debugging robot swarms in real time. To do this it will include a live video feed of the robot swarm, and connect to each of the robots wirelessly to retrieve internal data. This can then be combined using techniques similar to data and sensor fusion, to create graphical representations of the data which can be used to augment the video feed.
  
6: Background - Hardware
  The practical work for this project is being carried out at the York Robotics Laboratory on the Hes East campus, and we are hopeful that once completed this tool can be used in practical research work within the lab. For the first version of the system, the initial target platform will be the e-puck robot. This is a widely used, small robot platform for education and research purposes, often used for swarm robotics research. For this projects the e-pucks have been configured with a Linux extension board, which runs a Debian-based operating system on an ARM9 chip, and a WiFi adapter. In this configuration the ARM chip controls the robot's higher level decision making and processing, and interaces with the robots default dsPIC chip which handles the low level sensor and actuator control. The YRL has some existing infrastructure in the form of a robot arena equipped with a machine vision camera for doing robot tracking. This project incorporates this hardware, and uses the ARuCo marker based tag detection system to track the robots within the video feed, in order to situate and orient the graphical overlays.
  
7: Backtground - Hardware Images
  Here we have some images of the hardware, including the e-puck robot, the linux extension board, the ARuCo tracking tags, and the robot arena and tracking camera set up.
  
8: Design - System Architecture
  This diagram shows the general system architecture. The robots will communicated data regarding their internal variables, such as state, sensor readings, and other user defined variables, to the application via WiFi. The application back-end handles the server role. The tracking camera will send the video feed to the application via ethernet as raw image frames. The application handles running the tag detection algorithm to track the robots' positions and orientations. This data is combined, and the graphical overlay generated, and then presented to the user via the application GUI.
  
9: Design - Software Architecture
  The software has been designed following a Model View Controller architecture pattern. The model layer stores all the data, including data about each individual robot. The view layer displays this data through the UI and the augmented video feed, and handles user inputs. The controller layer sits in between the two and controls all the data processing. This includes receiving data from the robots and the images from the camera, running the tracking algorithm, updating the data model with the latest information and passing data to the UI for update. The application is serperated into threads to help with responsiveness and performance. The three key threads are shown here, with the two tasks likely to block exectuion (receiving data over the network and reading the camera) seperated from the main thread. Overall the software has been designed with modular, object oriented practices in mind, to improve extensibility and maintainability in the future.

10: Design - UI
  When designing the UI established application interface paradigms were considered, and the layout shown here was developed. Thel layout features three main panels with the larger window, each with a number of tabs. The larger panel, in blue, displays the video feed and graphical overlays, and the secondary tabs allow access to related settings. The right hand panel, in green, shows a list of the robots known to the system, and allows the user to select a specific robot. The secondary tabs are used to set up the networking to receive data from the robots, and configure the data logging. Finally the lower panel gives more detail on the specific robot, and has a tab for each data type reported by the robot. The interface has been created using the Qt GUI framework, and the image processing is handled using the OpenCV library.
  
11: Progress - Application
  The application is being implemented in C++, as this offers low level control and good performance with native object-oriented features, as well as being the native language for both Qt and OpenCV, and already widely used within the YRL for other systems. The core of the application has been implemented, including robot tracking and video feed augmentation as shown here...
  
12: Progress - Application
  ... and data display for several data types, as shown here. This includes custom user data. All of these data types have been incorporated into the data model. The code for receiving data from the robots via WiFi has also been implemented, as well as the controller layer code for inserting this data into the model.
  
13: Progress - Robot Code
  On the robot side a small single-class API has been written which handles some basic networking functionality to send data to the application as UDP packets. This is also implemented in C++, and is designed to be platform agnostic and portable to other robot platforms running Linux operating systems.
  
14: Future Work - Remaining Features


15: Future Work - Testing and Evaluation Strategy
  A portion of the remaining work is to properly test the system. This will involve manually testing the user interface, as well as potentially using unit testing techniques to verify the data model and data processing elements. Some verification testing of the system as a whole will also be necessary. Once tested the system will be evaluated through observation of user trials. This will involve a user monitoring a swarm while it enacts a common swarm behaviour, and attempting to locate a deliberately planted bug or issue. This will then be followed by a questionnaire about the users experience of the system, including how easy it was to use, how intuitive the interface was, and whether they believed it to be beneficial in completing their task.
  
16: Potential Future Development
  An initial user survey carried out on a number of potential users from within the YRL showed that there was an interest in the system, and the potential to develop it into a full swarm robotics platform. This would involve a number of extra features, such as recording and exporting data and video from experiments, performing data processing and macro-level swarm analysis, and bi-directional communication with the robots, so that instructions could be sent such as start and stop commands, or high level task directions. The system could also be extended to interface with more robot platforms. This might require that support for Bluetooth communication be implemented in addition to WiFi. Another, more grand development could be to port the whole system to a true augmeneted reality platform, such as microsoft hololens, and full 3D graphical augmentation. This would allow for more immersive and intuitve human-robot interaction.
  
17: Conclusion
  In conclusion, to summarise what we've just heard, this is a system that has been developed to tackle some of the difficulties in debugging robot swarms using a graphical, real time approach. The design focused on modularity and extensibility, as well as well established practices such as Model View Controller architecture and Object Oriented programming. The implementation stage is very nearly complete, and formal testing and evaluation will begin soon. The system has the potential to be developed in future into a fully fledged swarm robotics platform, which could be useful within the YRL and in the wider robotics community.
  
18: Thanks
  Thank you very much for listening, I would be happy to answer any questions you have.
  
  
  